{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "ROOT_DIR = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyntraScraper:\n",
    "    def __init__(self):\n",
    "        self.NUM_PAGES=6338\n",
    "        self.headers={\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}\n",
    "        \n",
    "    def parseData(self, response):\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        scripts = soup.find_all('script')\n",
    "\n",
    "        for script in scripts:\n",
    "            # print(script)\n",
    "            if 'window.__myx = ' in script.text:\n",
    "                json_str = script.text.split('window.__myx = ')[1]\n",
    "                return json.loads(json_str)\n",
    "            \n",
    "    def saveData(self, page, scraped_json):\n",
    "        with open(f'scraped/{page}.json', 'w') as f:\n",
    "            json.dump(scraped_json, f)\n",
    "\n",
    "    def parse_and_dump(page, response):\n",
    "        scraped_json = parseData(response)\n",
    "        saveData(page, scraped_json)\n",
    "        \n",
    "    def scrape_all(self):\n",
    "        for page in range(1, self.NUM_PAGES + 1):\n",
    "            print(\"Scraping: \", page)\n",
    "            r = requests.get(f'https://www.myntra.com/womens-western-wear?p={page}', headers=self.headers)\n",
    "            parse_and_dump(page, r)\n",
    "            print(\"Saved:    \", page)\n",
    "            time.sleep(0.4)\n",
    "            \n",
    "class MyntraProcessor:            \n",
    "    def get_scraped_products(self):\n",
    "        SELECTED_PRODUCT_KEYS = ['landingPageUrl', 'productId', 'product', 'productName', 'rating', 'ratingCount',\n",
    "        'isFastFashion', 'brand', 'searchImage', 'sizes', 'gender', 'primaryColour', 'additionalInfo', 'category',\n",
    "        'price', 'articleType', 'subCategory', 'masterCategory']\n",
    "\n",
    "        all_products = []\n",
    "        for file in os.listdir('scraped'):\n",
    "            try:\n",
    "                data = json.loads(open(f'scraped/{file}').read())\n",
    "                products = data['searchData']['results']['products']\n",
    "                filtered_product = [{key: product[key] for key in SELECTED_PRODUCT_KEYS} for product in products]\n",
    "                all_products.extend(filtered_product)\n",
    "            except Exception as e:\n",
    "                print(f'Failed {file} : {e}')\n",
    "        return all_products\n",
    "\n",
    "    def write_to_csv(self, filename):\n",
    "        scraped_products = get_scraped_products()\n",
    "        pd.DataFrame(scraped_products).to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstagramScraper:\n",
    "    def setup_dirs(self):\n",
    "        if not os.path.exists(ROOT_DIR + 'scraped'):\n",
    "            os.mkdir(ROOT_DIR + 'scraped')\n",
    "        if not os.path.exists(ROOT_DIR + 'scraped/instagram'):\n",
    "            os.mkdir(ROOT_DIR + 'scraped/instagram')\n",
    "        \n",
    "        if not os.path.exists(ROOT_DIR + 'related_usernames'):\n",
    "            os.mkdir(ROOT_DIR + 'scraped/instagram/related_usernames')\n",
    "        \n",
    "    def __init__(self, usernames_to_scrape, max_usernames_to_scrape=1000):      \n",
    "        self.scraped_usernames = set(os.listdir(ROOT_DIR + 'scraped/instagram'))\n",
    "        self.usernames_to_scrape = []\n",
    "        self.max_usernames_to_scrape = max_usernames_to_scrape\n",
    "        \n",
    "        self.headers ={\n",
    "            # this is internal ID of an instegram backend app. It doesn't change often.\n",
    "            \"x-ig-app-id\": \"936619743392459\",\n",
    "            # use browser-like features\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9,ru;q=0.8\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "            \"Accept\": \"*/*\",\n",
    "        }\n",
    "        \n",
    "        self.profile_url = 'https://i.instagram.com/api/v1/users/web_profile_info/?username={username}'\n",
    "\n",
    "        \n",
    "    ## Scrapes profile and saves it. \n",
    "    ## Returns usernames of related public profiles. \n",
    "    def scrape_profile(self, username):\n",
    "        try:\n",
    "            resp = requests.get(url.format(username=username), headers=self.headers, timeout=5)\n",
    "            assert resp.status_code == 200, f'Failed to fetch {username} : {response.content}'\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return []\n",
    "        related_usernames = [x['node']['username'] for x in resp.json()['data']['user']['edge_related_profiles']['edges'] if not x['node']['is_private']]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'scraped/instagram'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mset\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscraped/instagram\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'scraped/instagram'"
     ]
    }
   ],
   "source": [
    "set(os.listdir('scraped/instagram'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello:  True\n"
     ]
    }
   ],
   "source": [
    "print('hello: ', os.path.exists('scraped'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
